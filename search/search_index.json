{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Fish&amp;Feats","text":""},{"location":"#installation","title":"Installation","text":"<p>FishFeats is distributed as a pip module and can be installed normally in a virtual environment with:  <code>pip install fishfeats</code></p> <p>See more details in the installation page, especially if you are not familiar with installing a pip module.</p>"},{"location":"#usage","title":"Usage","text":"<p>You can launch <code>fishfeats</code> in napari by going to <code>Plugins&gt;fishfeats&gt;Start</code>. It will open a file dialog box asking you to select the image that you want to analyze. Possible input formats are currently <code>.tif, .czi, .ims</code>. For other formats, you can first open the image within napari and then start FishFeats by selecting <code>Plugins&gt;fishfeats&gt;Start from layer(s)</code>. See here for more information.</p> <p>Then the image will be displayed, with the different channels shown as separated layers on the left panel.</p>"},{"location":"#outputssetup","title":"Outputs/Setup","text":"<p>All the outputs of <code>fishfeats</code> will be saved in the folder called <code>results</code> that will be automatically created in the folder containing your image. If you run <code>fishfeats</code> again on the same image, the program will look into that folder for already saved files, so that you can load previous files and don't have to redo all the steps from scratch.</p> <p>At each step, the parameters that you used for your current image are saved in the associated configuration file (in the <code>results</code> folder, the file <code>yourimagename.cfg</code>) and will be reloaded each time you redo the same step.</p> <p>From version 1.2 of FishFeats, for all steps, measures are saved in the same file in the <code>results</code> folder, called <code>yourimagename_results.csv</code>. You can open this file out of the pipeline with any software for tabular data reading/analysis (Excel, R..) and analyse/extract the desired columns. </p>"},{"location":"#main-features","title":"Main features","text":"<p><code>fishfeats</code> proposes several analyses steps in the main interface:</p> <ul> <li>Image scalings: set the global parameter of the image to analyse (scalings, channels)</li> <li>Segment cells: segment/load/correct the cell apical contours in 2D</li> <li>Segment nuclei: segment/load/correct the nuclei in 3D.</li> <li>Separate junctions and nuclei: if the junctions staining and nuclei staining are in the same channel, to segment them it is necessary to separate them before with this step.</li> <li>Segment and assign RNAs: segment/assign/correct/measure the RNAs in one or more RNA channel.</li> <li>Classify cells: manually classify the segmented cells with a user defined criteria (eg \"PCNA or not\"). Can be automatically prefilled then manually corrected.</li> <li>Measure cytoplasmic staining to measure the intensity of one or more channels in each segmented cell around the surface.</li> <li>Measure nuclear staining to measure the intensity of one or more channels in each segmented nucleus.</li> <li>Measure RNA staining to measure the intensity of one or more channels in each segmented RNA.</li> </ul> <p></p> <p>When you open a new image, the plugin will directly go to the first mandatory steps of fixing the image scales and channels (image scalings). To guide you through the steps, we propose typical pipelines that can be followed with flowcharts here. Fish&amp;Feats allows flexibility in its usage, so that it can be used for different image analysis tasks, so we don't enforce any order for the proposed action, but a few depends on other being done before. Please refer to the flowchart to see the dependencies.</p> <p>Finally, to allow for more flexibility, it is possible to go back and forth between steps.  For example, after segmenting the cells, if you work on classifying the cells and spot an error in the segmentation that you missed previously, you can go back to the segmentation correction and back again to the classification afterwards without having to restart it from scratch. Just save regurlarly your data to be able to reload it.</p> <p>The parameters that you enter in the interface for each step are saved in a configuration file (.cfg) so that when you reload an image, all parameters will be set to the last value used.</p>"},{"location":"#general-shortcuts","title":"General shortcuts","text":"<p>For each step, FishFeats proposes shortcuts to make its use more agreable/user-friendly, aditionnaly to the ones already proposed by Napari. The specific shortcuts are indicated in the text overlay showed at the top left side of the view. </p> <p>You can always press h to show/hide these help messages.</p> <p>A few other shortcuts are always available:</p> Shortcuts Napari default shortcuts go to <code>File&gt;Preferences&gt;Shortcuts</code> to see the list of available shortcuts, associated with each kind of layers h show/hide help message Ctrl-v Activate/desactive vispy visualisation mode in 3D. This mode allows you to control the visualisation angle, but can inactive some selection tools. F1 Show/hide the first layer (from the list of layers in the left bottom part of the window, starting from the bottom). By default, the first layer should be your input image first color chanel, called <code>originalImageChanel0</code> in FishFeats. F2 Show/hide the second layer, F3 for the third layer..."},{"location":"#pipeline-steps-overview","title":"Pipeline: steps overview","text":"<p>To guide users through the steps, we propose typical pipelines that can be followed with flowcharts here</p>"},{"location":"#hierarchical-clustering","title":"Hierarchical clustering","text":"<p>You can launch this analysis with <code>Plugins&gt;fishfeats&gt;Hierarchical clustering</code>. It will perform hierarchical clustering on a set of columns (that contains RNA counts for example) and show the resulting clustering on the segmented cells.  See Hierarchical clustering for more infos.</p>"},{"location":"#issues","title":"Issues","text":"<p>A list of encountered errors and their solution is given here</p>"},{"location":"3d-cell-positions/","title":"3D cell positions","text":"<p>Estimate or correct the Z position of each cell</p> <p>Choose Cells:3D position in the main action choice interface to visualize and edit cells Z position.</p> <p>Epithelial junction are segmented on the 2d projection of the junction staining. From this segmentation, each cell is then back-projected in the 3D junction staining to estimate its position in Z. This information is saved in a file called imagename_cellsZ.csv. You can directly modify this file and put the desired Z value in the corresponding cell row, or use the plugin interface to modify it manually.</p> <p>The pipeline draws the cells at their respective Z-position. You can visualize it in 3D by switching to the 3D mode at the left bottom of the napari interface, but editing should be done in the 2D view. </p>"},{"location":"3d-cell-positions/#z-map-calculation","title":"Z-map calculation","text":"<p>If the Z positions of the cells are mostly wrong, you can recalculate them by clicking on <code>recalculate zmap</code>. This back-projects locally the junction staining by finding the Z that has the most similarity with the projected staining.  This map precision depends on the parameters: * <code>zmap resolution</code> which is the size (in pixels) of sliding windows on which the Z is estimated. A lower value will allow for more resoluted values but will take longer to calculate. * <code>zmap localsize</code> is the size of the neighboring area used to calculate the best similarity of the projected and original staining. If it's too small, it can influenced by local noise while too large values doesn't account for tissue high curvature (where the Z of the tissue varies rapidly).</p>"},{"location":"3d-cell-positions/#individual-cell-correction","title":"Individual cell correction","text":"<p>When the global Z positions are correct, you can then edit each individual Z position. For this: * Right-click on the cell to edit. The value of its label will be put into <code>cell label</code> parameter * Go to the correct Z slice * Control+left-click to place the selected cell at the current Z position (visible slice). The value of the Z will be put into the <code>place at z</code> parameter and you will see the cell appaearing in the current slice. <p>These steps should be done in that order for the option to function correctly.</p> <p></p> <p>When all corrections are done, save the new positions by clicking <code>Save updated cells</code>.</p>"},{"location":"3d-cell-positions/#export-3d-segmentation","title":"Export 3D segmentation","text":"<p>This option exports a 3D stack of the cell labels, placed in their 3D position as a <code>.tiff</code> file. The exported file will be saved in the <code>results</code> folder, called imagename_cells3D.tif. This file is not necessary for any other step of the pipeline, so click it only if you want to have the 3D segmentation exported to use in another software.</p>"},{"location":"Add-grid/","title":"Add grid","text":"<p>Add a grid to the viewer</p> <p>To add/show/hide a grid on the viewer, press g. Or select <code>Misc:Add grid</code> in the main pipeline interface</p>"},{"location":"Add-grid/#grid-set-up","title":"Grid set-up","text":"<p>When creating the grid, if you press g or select the <code>Add grid</code> option, the parameters to setup the grid will appear in the right panel of the viewer.</p> <p>You can specify the number of lines and columns.  By default, the name of each cell of the grid is displayed in the center of the cell. You can disable this by unselecting <code>display text</code> in the top left panel of the <code>FisherGrid</code> layer (which should be selected). In this panel, you can also modify the width of the grid lines and its color.</p> <p></p>"},{"location":"Add-grid/#grid-display","title":"Grid display","text":"<p>Once the grid has been created, you can always switch between showing it or hiding it by pressing g. </p>"},{"location":"Associate/","title":"Associate to cell","text":"<p>Associate segmented apical cell with segmented nuclei</p> <p>Choose Nuclei: Associate to cells in the main pipeline interface to run this step. The cell junctions and the nuclei must have been segmented/loaded before to do this step.</p> <p>Once you have separately segmented the cells (their surface) and the nuclei, this step allows for the cells and nuclei to be paried together to define the full cell. At the end of it, you will have for each segmented cell a corresponding nucleus (indicated by its label) or none if no close enough nuclei corresponded.</p> <p>You have to first run an automatic association that will try to find the best pairing (nucleus, cell) combinations. Then you will get a step to manually correct the association. </p>"},{"location":"Associate/#computing-association","title":"Computing association","text":"<p>The algorithm to do the association is the Hungarian algorithm that finds an optimal pairing of nuclei and cells based on a linking cost. This cost, the distance between the nuclei and the cell is taken as the distance in 3D between the cell surface center and the nuclei center , plus the distance in the (x,y) plane between those two centers (this addition allows to penalize more for distance in XY as nuclei are more vertically below the cell surface).</p> <p>The parameter <code>distance toassociate micron</code> defines a threshold distance (the distance3D+distance2D) above which a nucleus is considered to far to correspond to the cell. This reduces the number of candidate nuclei and the searching range to do the association, thereby speeding-up the computation, but may leave some nuclei unassociated with cells.</p> <p>Once the association have been computed, you obtain two new layers on the left panel, <code>CellNuclei</code> and <code>CellContours</code> that contain the cell labels and the updated cell nuclei label. The nuclei labels are changed so that their labels match with each associated cell (thus a nuclei and cell with the same value are associated). In general, the colors should match as well, but it is not always the case, so check the label value by hovering over the nuclei and right-clicking on the cell surface to get the cell label.</p>"},{"location":"Associate/#manual-correction","title":"Manual correction","text":"Shortcut/options <ul> <li>To see the value of a nucleus label, you can put the mouse on top of it and you will see its label on the bottom left panel (except if vispy visualisation mode is on, disable it to get the value). Or you can double-left click on it to put its value on the <code>nucleus</code> parameter in the <code>CellNuc association</code> panel.</li> <li>Right-click on a cell to get its label and have it put to the <code>cell</code> parameter in the <code>CellNuc association</code> panel.</li> <li>Click on <code>Associate now</code> to associate the nucleus with label <code>nucleus</code> with the cell with label <code>cell</code>. If another nucleus has already the same label of the cell to associate with, its value will be changed to the maximum label + 1. Shortcut: press on c to do the association. The selected nuclei should change its color and label value to the <code>cell</code> one.</li> </ul> <p>Additional options:</p> <ul> <li><code>show_cellnames</code> add the value of each cell label in the image.</li> <li><code>sync cellsNuclei</code> synchronize the <code>CellNuclei</code> and <code>CellContours</code> layers, so that visualisation options set on one layer are also set on the second layer. This is usefull to see only one label (with the <code>show selected</code> option in the top left panel) in both layers at the same time. </li> </ul> <p></p>"},{"location":"Associate/#outputs","title":"Outputs","text":"<p>When the association is finished and corrected, save the two label files to be able to reload them later. By default, the cell surface will be saved in a 2D label image called <code>_imagename_\\_cells2D.tif</code> in the <code>results</code> folder,the same as for the junctions segmentation. The nuclei will be saved as a 3D label stack, containing each nuclei with its associated label in a file called <code>_imagename_\\_nuclei.tif</code>.  The <code>save also3D junctions</code> option saves another file of the cell surface labels at their corresponding slice (Z) for each cell.</p> <p>Click on <code>save association</code> to save these files and you can click on <code>Association done</code> to finish this step.</p>"},{"location":"Classify-cells/","title":"Classify cells","text":"<p>Classify cells with a user defined criteria</p> <p>Select Measure:Classify cells in the main pipeline step (1) </p> <ol> <li>You can also launch it out of the pipeline in <code>Plugins&gt;FishFeats&gt;Classify segmented cells</code></li> </ol> <p>See the typical workflow of this step here</p> <p>Cells must have been created/saved before to use this step</p> <p>The file named imagename<code>_cells2D.tif</code> should have been saved, or the <code>get cells</code> step should have been performed in the main pipeline before.</p> <p>When you start this step, you get an interface with a table containing all the cells and all the features (classifications) already done.  You can click on one cell to see it in the image.</p> <p>In the second window of the interface, you can add a new feature/classification to do, either by writing a new feature name to create a new one, or by selecting an already present one in the list to edit it. Choose a feature name that will be relevant for you (eg. \"PCNA\", \"DoubleNucleus\", \"SuperCell\"...).  The program will add \"Feat_\" in front of the name of the feature to indicate that it is a feature.</p> <p>Click on <code>Do feature</code> to open the interface to choose how you want to initialize your classification.</p> <p></p>"},{"location":"Classify-cells/#classification-prefilling","title":"Classification prefilling","text":"<p>When you click on <code>Do feature</code>, it will either reload the feature if it was already present or open a parameter interface to choose how to initialize the feature.</p> <p></p> <p>In the first line, the interface shows the feature name that you have entered.</p> <p>You can choose the number of possible classes for this feature (<code>Nb classes</code> parameter)(1). For example, if the feature can only be positive (=2) or negative (=1), the number of classes will be 2.</p> <ol> <li>Note that you can also change it later by clicking <code>Add one class</code> in the edit interface. </li> </ol> <p>Then you can select the method to use to prefill the classification automatically (you will be able to manually edit it afterwards). </p> <p>You can either have a prefilled classification with all the cells in the same class (<code>Initialize all cells at 1</code>), based on a thresholding of one chanel (<code>from projection+threshold</code>), or based on the position of cells on edges or not (<code>Boundary cells</code>). </p> <p>Click on <code>Create new feature</code> to launch the classification of your new feature. It will add a new layer, called <code>Feat_</code>featurename<code>Cells</code>, prefilled according to the selected method. In that layer, one color corresponds to one class so if <code>nb classes</code> is 7, you can have 7 colors.</p> <p>You now have the possibility to manually edit the classification, with the <code>Feat_</code> featurename parameter interface and to see the table of cells with their corresponding classification with the <code>Features table</code> interface.  Click on <code>Update/save table</code> button to update the displayed table in the <code>Feature table</code> window and save it in the <code>results.csv</code> file. If you do another feature later (running again the <code>Edit/Add feature</code> interface), it will be added to this table so you can add additional features to the analysis.</p>"},{"location":"Classify-cells/#empty-prefilling","title":"Empty prefilling","text":"<p>The classification can be prefilled automatically if your classification is binary (\"yes\" or \"no\") and depends on the intensity in one chanel of the image. </p>"},{"location":"Classify-cells/#from-intensity-projection","title":"From intensity projection","text":"<p>Choose the method <code>from projection+threshold</code> and the corresponding chanel <code>proj chanel</code>. It will make a 2D projection of this chanel, and classify as \"yes\" (2) cells that have at least <code>threshold_areaprop</code>% of its pixel brighter than: <code>mean(intensity) * threshold_frommean</code>.</p> <p>You can then see the table with the automatically calculated feature and manually edit the automated classfication by using specific shortcuts.</p> <p></p>"},{"location":"Classify-cells/#boundary-classification","title":"Boundary classification","text":"<p>With this option, you can choose to automatically classify the cells according to whether they touch the border of the image (and thus might not be complete), or are on the edge of the tissue (no neighbors in one side), or next to a big hole in the tissue. By selecting <code>Boundary cells</code>, you can choose if you want to classify the cells that touch the border of the image <code>Image border</code> and/or on the edges (of tissue or holes) <code>Tissue boundary</code>.  The cells will be classified as 1 if they are not a border or a boundary, 2 if they are a boundary cell, and 3 if they are a border cell.</p> <p></p>"},{"location":"Classify-cells/#load-classification","title":"Load classification","text":"<p>If you select a feature name from the proposed list on the feature name parameter and click on <code>Do feature</code>, it will automatically load the previous classification (that is saved and loaded from the file imagename<code>_results.csv</code>). You can edit it directly.</p>"},{"location":"Classify-cells/#manual-editing","title":"Manual editing","text":"<p>To modify the classification of some cells, you can set the current value to assign with the <code>Class value</code> parameter. This parameter can take a value from 1 to the <code>nb classes</code> parameter that you chose previously in the <code>Do feature</code> interface (the number of classes).  You can increase the maximum number of classes to add a new one by clicking the button <code>add one class</code> in the right-side of the interface.</p> <p>You can change the current value of this <code>Class value</code> parameter by sliding the bar in the interface, or by pressing i to increase its value or d to decrease it.</p> <p>Also, if you right-click on a cell, you can set it to the cell's class. It will automatically set-up the <code>Class value</code> parameter to the classification of the selected cell. </p> <p></p> <p>To change the class of a cell and set it to the current value of <code>Class value</code> parameter, press Control+left click on the cell.  It's color will be udpated automatically after the click.</p> <p>When you click on <code>Update/save table</code>, the current classification will be saved (in the filename<code>_results.csv</code> file along with the other results) and the displayed table in the <code>Feature table</code> window will be updated.  The current step is not stopped, so don't hesitate to save regularly.</p> <p>You can then click on <code>Feature</code> featurename <code>done</code> to close the interface of this feature and do another feature or finish this step. It will remove the feature layer, but the results of the classification will still be present in the features table.</p> <p>If you want to export the view of the classified cells, click on <code>export feature image</code>. It will save it in a file called imagename<code>_feat_</code>featurename<code>.png</code> in the <code>results</code> folder.  </p>"},{"location":"Classify-cells/#features-table","title":"Features table","text":"<p>All the results of the classified features are summarized in the <code>Feature table</code>. Each row is one segmented cell and each column the features that have been defined.  The table will be saved in the results folder in the results file called imagename<code>_results.csv</code> .  If the plugin is closed and open again on the same image, the features will be automatically reloaded. You can edit them by loading the corresponding feature. </p> <p>If you click on <code>Stop and Save</code>, this feature table will be closed, and all opened feature layers and interfaces will be closed.</p>"},{"location":"Get-RNAs/","title":"Segment&assign RNAs","text":"<p>Segment RNAs and assign them to their cell</p> <p>Choose RNA:segment&amp;assign in the main interface to run it</p> <p>See the typical workflow of this step here</p> <p>This step will first propose you to segment the RNA from one of the channels then to assign it to its corresponding cell.  The cells must have been created before to do this assignment step, either with the Cells segmentation step, or loaded from previous file (by default the cells should be reloaded).</p> <p>Since version 1.1.23, there is also an option to measure the intensity of a given layer/channel in the segmented spots (e.g. to measure if the segmented spots are inside nuclei, or correlates with some immuno-staining).</p>"},{"location":"Get-RNAs/#rna-segmentation","title":"RNA segmentation","text":"<p>First, you have to select which channel of the images to analyze for RNA detection, by setting the <code>rna channel</code> parameter to the corresponding <code>originalChannel*</code> number.</p> <p></p> <p>Then, the segmentation of the RNA dots can be performed with Big-fish (Imbert et al. 2021). To correctly detect dots, <code>big-fish</code> needs the average size of an individual spot. As the spot resolution can be different in the Z-direction, the size of the spot should be specify both in XY and in Z. Put the average spot radius in the XY plane, in nanometers in the <code>spotXYRadiusNm</code> field, and the average spot radius in Z in nanometers in the <code>spotZRadiusNm</code>.</p> <p>Big-fish filters the detected spots based on their mean intensity with a threshold. If you select <code>automatic threshold</code>, Big-fish will estimate and use this threshold. Otherwise you can specify a value in the <code>threshold</code> field. Note that after you run it one time on automatic mode, the value of the Big fish estimated threshold will appear in the <code>threshold</code> field, so that you can vary it around this value if the results are not satisfying.</p> <p>Big-fish can sometimes find spots in the first and last slices when there is no real spots or signals. The option <code>removeSpotInExtremeZ</code> allows to get rid of these artefact spots. Un-select it if you have RNAs spots that you want to analyze even in the first or last slices.</p> <p>When all the spots will have been detected, <code>fishfeats</code> will display them in a new Spots layer. To choose the size at which the spots will be displayed, change the <code>drawing spot size</code> parameter.</p> <p>If the resulting dots do not correspond well with the RNA spots in your channels, you can run this step again changing either the dot size parameters or the threshold value.</p> <p>You also have an option to load a previous segmentation if you already segmented the RNA in this channel. When you select this option, the <code>load file</code> parameter will appear and let you choose the corresponding file to load.</p> Applying preprocessing <p>In some cases, you might need to apply preprocessing to improve the results. If you want to apply other preprocessing that are not yet available in FishFeats, you can either work with another software with the required softwares and save the preprocessed nuclei in the input image, or work other napari plugins and use the option to start FishFeats from already opened layers.  You can also contact us (filing an issue in this repository) to ask for the possibility to add a given preprocessing algorithm in FishFeats</p>"},{"location":"Get-RNAs/#rna-assignment","title":"RNA assignment","text":"<p>Assigning RNAs corresponds to attributing to each dot the same label as the cell we consider that it belongs to. Several methods can be used for an automatic selection, and manual assignment or correction can also be done.</p>"},{"location":"Get-RNAs/#assignment-methods","title":"Assignment methods:","text":"<ul> <li> <p><code>Projection</code>: assign each RNA to a cell by projection in Z. Thus, the assignment depends only on the X,Y position of the dot and on the junction segmentation. </p> </li> <li> <p><code>ClosestNucleus</code>: assign each RNA to the cell of which the nucleus is the closest to the dot in 3D. The distance is calculated from the closest point of each nuclei to the RNA spot. </p> </li> <li> <p><code>MixProjClosest</code>: assign each RNA either to the closest nuclei if the RNA spot is deep in z (closer to the nuclei) or by projecting it to the apical cells if the spot is close to the surface. </p> </li> <li> <p><code>Hull</code>: to update</p> </li> <li> <p><code>FromNClosest</code>: assign each RNA according to its n-closest RNA from other chanels already assigned. This finds the n-closest points (n is defined by the <code>nb closest rnas</code> parameter) and assigns to current RNA spot to the most represented cell among these neighboring points. You can select which reference RNA points to use for this (already assigned channels). In the <code>reference rna channels</code>, the list of already assigned RNA channels is given, and you can select as many as these channels as you want.</p> </li> </ul> <p>For example, in the image below, 2 channels (1 and 2) have already been assigned and are by default pre-selected to use for the assignment of the 3rd channel. Thus by keeping these parameters, for each point of channel 3, the program will look at the 10-closest points from channels 1 and 2. It will then assign to this point the cell identity that is the most present among these 10-closest points. If the majority of points are unassigned, the current point will also be unassigned.</p> <p></p>"},{"location":"Get-RNAs/#additional-parameters","title":"Additional parameters","text":"<p>If you select the <code>show advanced parameters</code> option, you can also choose additional parameters to control the assignment method:</p> <ul> <li><code>limit distance to assign micron</code>: to apply a threshold of distance at which a point is still assigned to the corresponding cell/nucleus. When the calculated distance is above this threshold (in microns), the point will be unassigned.</li> <li><code>assign when above cells</code>: if selected, then RNA dots that were found above the cells (so not in the cells) can still be assigned to a cell with the selected method. Otherwise, only spots that are below the apical cells surface will be assigned. If selected, you can also choose the <code>nb z keep above</code> parameter to control until how many Z slices above the cells RNA points are still assigned.</li> <li><code>filename</code> is the name of the file on which the RNA segmentation and assignement will be saved. It is advised to keep the default name as it is also the one that the program will look for to load it.</li> </ul> <p>Click on <code>apply assignment</code> to launch the assignment method on the segmented dots. The dots will be all white during the calculation of the assignment. When the computation will be finished, they will be colored by their assigned cell. Dots with a maroon color, labelled \"1\" are unassigned, i.e. no corresponding cell was found.</p>"},{"location":"Get-RNAs/#rna-manual-correction","title":"RNA manual correction","text":"<p>To correct RNA assignment, select the point(s) to change the assignment value, put the value (label) of the cell to assign it to in the <code>assign points to cell</code> parameter field and click on <code>assign selected points</code>. </p> <p></p> <p>Options/shortcuts to correct the RNA assignment:</p> Shortcut/options <p>See napari Point layer documentation for more information on the point edition tools available by default in napari (and accessible in the top left panel of the interface)</p> VisualizationPoint (RNA) editing F1,F2,F3... Show/Hide the first, 2nd, 3th.. layer (ordered in the bottom left panel from bottom to top) v Show/Hide cell contours layer d Increase/Decrease the RNA layer opacity l Show only the currently selected cell contour (or reset to show all cells). The layer \"CellContours\" must be visible o Show only the selected points. Move the Display point size bar to reset it Get the label of the cell under the click and set the current assignment value ot it u Select all unassigned points s Select all points with current assignment value a Select all visible points c Assign current assignment value to all selected points Ctrl+ To select points by drawing a rectangle around them while keeping the mouse clicked. Other points can be added to the current selection by doing the same elsewhere Ctrl+ Unselect the points Alt+ Zoom on the clicked position Alt+ Unzoom Shift+s Shuffle the colors of the cells and points <p>The program locks the layers so that it is not possible to remove them by accident. It will print an error message if this happens. The lock will be released when clicking on <code>RNA* done</code>.</p> <p></p>"},{"location":"Get-RNAs/#point-display","title":"Point display","text":"<p>To further help with the manual correction of spots, <code>FishFeats</code> proposes an option to change the point size display.</p> <p>Check the <code>Point display</code> box to expand its content in the <code>Edit RNA</code> interface on the right side of the window.</p> <p>You can change the display size for all points by sliding the <code>Display point size</code> parameter bar, but you can also display some points at the current size, and hide other points by displaying them very small. For this select the criteria to display a spot as big or very small, in the <code>Point size from:</code> parameter, and select the desired option. The point can be displayed differently according to the intensity inside the point (<code>point intensity</code>) or from their assignment score (<code>assignment score</code>) which refers to the certainty of their assignment.</p> <p>Click on <code>Reset point display</code> to display all the spots with the same size again, controlled by the <code>Display point size</code> parameter.</p> <p></p>"},{"location":"Get-RNAs/#saveload-rnas","title":"Save/load RNAs","text":"<p>When you click on <code>save and quit RNA*</code>, it will save the current point layer in a <code>.csv</code> file saved in the <code>results</code> folder called imagename<code>_RNA*.csv</code> (here * is the number of the corresponding channel).  The file contains the position of each spot and its assignment. The counts of the RNA of this channel in each cell will be added to the results file in a new column. The editing interface of the current RNA will be closed so that you can go to the next RNA.</p> <p>To reload/continue the assignment later on, you can load this file in the segmenting RNA step by choosing <code>load file</code> in the segmentation method choice parameter.</p> <p>When you click on <code>save RNAs</code>, it will automatically save all the RNA channel that have been done and udpate the results saved as well. You can click this button at any time during the manual correction process, without stopping it. We recommend saving regularly in case something happens during the correction steps.</p>"},{"location":"Get-RNAs/#draw-rna-counts","title":"Draw RNA counts","text":"<p>This option, at the bottom of the \u00a0<code>Edit RNA*</code> interface, allows the cells to be visualized, colored by their number of assigned RNAs. </p> <p>Click on <code>Draw RNA* counts</code> to add a new (2D) layer containing the cells ans their number of RNAs. In each cell the intensity reflects the number of RNA assigned. </p> <p></p> <p>You can also save this image as a 2D file by clicking <code>Save drawn RNA* counts</code>.</p> <p>The RNA counts can also be found in the results file, in the corresponding <code>nbRNA_C*_MethodName</code> column. MethodName will be the name of the method used for the initial assigment. The results file can thus contain the counts from several assignment methods, but note that when you reload it, it will load only the last used method.</p>"},{"location":"Get-RNAs/#measure-intensity","title":"Measure intensity","text":"<p>Select the onglet <code>MeasureIntensity*</code> to start this option</p> <p>This option allows the intensity inside each segmented spot (RNA) in a chosen image channel or opened layer to be measured. It can also indicate for each spot if it is inside a segmented nucleus or not (you must have performed the nuclei segmentation before for that, see the nuclei segmentation step).</p>"},{"location":"Get-RNAs/#inside-segmented-nuclei","title":"Inside segmented nuclei","text":"<p>You must have done the nuclei segmentation before to be able to use this option.</p> <p>When clicking on <code>Measure points inside nuclei</code>, the plugin will evaluate for each segmented RNA spot if its center is inside a segmented nucleus or not. The spots will then be displayed with a color indicating if they are inside a nucleus (yellow) or not (purple).</p> <p></p> <p>This value will also be added to the <code>imagename RNA*.csv</code> file in which the RNA results are saved, in a column called <code>InsideSegmentedNuclei</code> when you click on <code>Save RNAs</code>. </p>"},{"location":"Get-RNAs/#measure-raw-intensity","title":"Measure raw intensity","text":"<p>At this step, usually only the <code>originalChannel*</code> channels are available, but you can open other layers (e.g. nuclei segmentation) if you want to measure it and click on <code>update layers</code> to have it added to the possible layers to measure.</p> <p>Choose the layer/image to measure in <code>From layer:</code> parameter in the interface.</p> <p>When you click, the intensity inside each spot (average of the pixels within a radius of 4 pixels of the spot position in XY) in the selected image will be measured. The spots will then be displayed with a color relative to their measured intensity.</p> <p></p> <p>When you save the RNA spots position (with the button <code>save RNAs</code>), a column with the measured intensity called <code>Int_layername</code> will be added in the resulting <code>.csv</code> file.</p>"},{"location":"Get-RNAs/#finish-rnas-analysis","title":"Finish RNAs analysis","text":"<p>When you have analyzed all the RNA channels in your image, you can quit this step by clicking on <code>RNAs done</code> in the right panel. This will close all the open RNA channels and parameters interface and get you back to the main pipeline step. </p>"},{"location":"Get-cells/","title":"Segment Cells","text":"<p>Segment the junction staining to detect the cells apical area.</p> <p>Choose Cells:Segment in the main interface to run it. (1)</p> <ol> <li>Nb: in previous version of FishFeats, this step is called <code>Get junctions</code> in the interface</li> </ol> <p>See the typical workflow of this step here</p>"},{"location":"Get-cells/#segmentation-process","title":"Segmentation process","text":"<p>1/ 3D-&gt;2D: The segmentation of the cellular junctions is performed in 2D. First the junction staining will be projected in a 2D plane by taking a local average of the slices around the maximum intensity signal. If the nuclei staining is in the same channel, the signals will be separated first. In that case, when you click on Cells:Segment the interface to separate junctions and nuclei will appear and you must execute it before to be able to do the segmentation. A layer called <code>2DJunctions</code> will appears when the projection will be finished/loaded.</p> <p>2/ 2D segmentation: The plugin proposes several options to perform the segmentation from the 2D images of junctions:</p> <ul> <li>Epyseg: tool for epithelial segmentation from Aigouy et al. 2020.</li> <li>CellPose: tool for cellular segmentation from Stringer et al. 2021.</li> <li>Load an already segmented file (should be a file of labelled cell). If you choose this option, the plugin will look for the labelled file of cell, named imagename<code>_cells2D.tif</code> in the results folder, but you can select another file.</li> </ul> <p>3/ Manual correction: When the computation of the segmentation is finished, <code>fishfeats</code> will show you the results in a <code>label</code> layer called <code>Junctions</code>. You can then perform manual correction if needed before saving the results.</p> <p>4/ 2D-&gt;3D: At the end of the process, when you click on <code>Junctions done</code>, the pipeline creates the cells from the segmentation. The shape will be the label shape and the cell position in Z will be back-projected into the junction staining. Thus this step can take a few minutes to calculate the back-projection. </p> <p></p> Applying preprocessing <p>In some cases, you might need to apply preprocessing to improve the results. If you want to apply other preprocessing that are not yet available in FishFeats, you can either work with another software with the required softwares and save the preprocessed nuclei in the input image, or work other napari plugins and use the option to start FishFeats from already opened layers.  You can also contact us (filing an issue in this repository) to ask for the possibility to add a given preprocessing algorithm in FishFeats</p>"},{"location":"Get-cells/#2d-projection","title":"2D projection","text":"<p>The junction staining will be segmented in 2D. </p> <p>If you have already calculated the projection previously or with another pipeline, you can load here the image of the projection and directly use it. Otherwise, the pipeline will calculate the projection by looking at local maximum intensities.</p> <p></p> <p>The projection will be displayed in a new layer, shown in white, and called <code>2DJunctions</code>. The pipeline will by default save the calculated projection in the <code>results</code> folder, except if you don't check the <code>save projection</code> option. When this step is done, you can now performs the segmentation of this projected image.</p>"},{"location":"Get-cells/#loading-the-projection","title":"Loading the projection","text":"<p>Click on <code>Load default</code> if you have already saved the projection with the default name (<code>yourimagename_junction_projection.tif</code>).</p> <p>If you have calculated the projection in an other software (e.g with LocalZProjector in Fiji), you can choose the file of the projected junction channel and load it directly.  If the option <code>save projection</code> is selected, the loaded file will be copied and saved in the main <code>results</code> folder, with the pipeline's default name for the projection.</p>"},{"location":"Get-cells/#calculating-the-projection","title":"Calculating the projection","text":"<p>The junction staining will be projected in 2D, by looking at the local maxima positions in Z in the neighboring of each pixels. The intensity around this local maxima position will then be projected in 2D for each pixel.</p> <p>You can directly try to click on <code>Project now</code> to calculate the projection with the default parameters. If the result is not satisfying, then several parameters can be tuned by checking the <code>Advanced</code> option: * Local size: size of the local window around each pixel to look for local maxima * Smoothing size: amount of smoothing of the projected pixels * Do local enhancement: performs local contrast enhancement with CLAHE method. This is useful if the illumination is quite variable in the image to uniformize it before segmentation * CLAHE grid size: if performing local enhancement, size of the grid use to locally improve contrast</p> <p>Click on <code>Project now</code> to recalultes with the new parameters.</p> <p>If this algorithm doesn't succeed even with tuning the parameters, you can use dedicated software to local projection as LocalZProjector and then load the results in <code>fishfeats</code>.</p>"},{"location":"Get-cells/#manual-correction","title":"Manual correction","text":"<p>The result of the segmentation is saved and displayed as labelled apical cells: each cell is assigned a unique number (the label) that is put in all the pixels inside the cell surface. The colors reflect the values (labels) of each cell. 0 indicates background pixels (no cell). In <code>fishfeats</code> the label <code>1</code> is reserved to indicate unassigned elements, so should not be used to label cells.</p> <p>To correct eventual segmentation errors, on the left top panel, you have the napari tools to edit labels and on the right panel additional <code>fishfeats</code> editing options.</p> Shortcut/options <p>See napari Label layer documentation for more information on the label edition tools available by default in napari (and accessible in the top left panel of the interface)</p> VisualizationLabel (cell) editing F1,F2,F3... Show/Hide the first, 2nd, 3th.. layer (ordered in the bottom left panel from bottom to top) <code>contour</code> Labels (cells) can be displayed as filled areas (put <code>contour</code> to 0) or only with the contour lines (<code>contour</code>&gt;0). <code>show selected</code> Display only the current label (the pixels which ahve the value that is currently acitve in the <code>label</code> field). L Show/Hide all the labels value as a text overlay. Or check/uncheck <code>show cellnames</code> in the right panel 5 Switch between zoom/moving mode V Show/hide the current layer (the <code>Junctions</code> layer) Ctrl+c/Ctrl+d Increase/Decrease the labels contour width (will be filled if reaches 0) 2 Select drawing modes (or click on ). When it's active, it will draw with the current <code>brush stroke</code> size 3 Select filling mode: it replaces a whole label at one by the current value () Select the label under the mouse. Or 4 to switch to picker mode, then click on it Ctrl+ Delete the whole label (cell) below the click Ctrl+ Merge two neighboring labels into one cell. You should keep the mouse button clicked when dragging from one to another. <code>Relabel</code> Reorder the cell names (labels) with consecutive values from 2 to the number of cells M Draw a new cell: select unused value and switch to drawing mode <code>preserve labels</code> If checked, other labels than the currently active one cannot be modified when you are drawing (so even if you touch them they will not be edited)"},{"location":"Get-cells/#save-corrected-results","title":"Save corrected results","text":"<p>To save the manual corrections, click on <code>save junctions</code>. It will save a file called imagename<code>_cells2D.tif</code> in the <code>results</code> folder that contains the labelled cells.</p>"},{"location":"Get-cells/#measures","title":"Measures","text":"<p>You can display a table of measurements of the cells position, area and label. Click on the button <code>Show measures</code> to perform the measurement. A new window containing the table of all the cells and their area will appear.</p> <p></p> <p>This table will be automatically saved when the cell segmentation is saved, in the imagename<code>_results.csv</code> output file and will be completed during the pipeline by other measurements (cytoplasmic measures, nuclei measure, RNA counts...)</p>"},{"location":"Get-cells/#junction-analysis-finished","title":"Junction analysis finished","text":"<p>When you have finished the segmentation and manual correction steps, click on <code>Junctions done</code> to quit this step and go back to the main step choices.</p> <p>If <code>save when done</code> is checked, the segmentation will be saved as a <code>.tif</code> file that can reloaded later, as well as the table of the cell coordinates and area in the imagename<code>_results.csv</code> output file.</p>"},{"location":"Get-nuclei/","title":"Segment Nuclei","text":"<p>Segment nuclei in 3D from the nuclei staining chanel</p> <p>To segment nuclei in 3D, choose the option Nuclei:Segment in the main pipeline interface.</p> <p>See the typical workflow of this step here</p> <p>For some images, preprocessing of the nuclei chanels (to smooth, denoise..) can improve the performance. In these cases, first execute the preprocess nuclei step.</p>"},{"location":"Get-nuclei/#segmentation-methods","title":"Segmentation methods","text":"<p>Two segmentation tools are proposed to perform the 3D segmentation:</p> <ul> <li>Stardist</li> <li>CellPose3D</li> </ul>"},{"location":"Get-nuclei/#stardist","title":"Stardist","text":"<p>Stardist run in 2D + 3D reconstruction.</p> <p>Each Z-slice will be segmented in 2D for individual nuclei with Stardist (Schmidt et al., 2018). Then the 3D nuclei will be reconstructed by associating the nuclei from each consecutive slices, either with the <code>Munkres</code> method (Hungarian algorithm, optimization of the pairing of the ojects) or with the <code>overlap</code> method (associating overlapping object, faster).</p> Stardist parameters <ul> <li><code>probability threshold</code>: threshold of Stardist output probability to keep a detected nuclei. Increase it will decrease the number/size of nuclei found.</li> <li><code>nuclei overlap</code>: Stardist parameter of how much nuclei overlap in general. Increasing it to split more nuclei, decreasing to have bigger objects.</li> <li><code>association method</code>: how to reconstruct 3D nuclei from the 2D slice nuclei, either by combinataion (optimization of the distance between nuclei in two consecutives slices), <code>Munkres</code> method, or by associating nuclei from consecutive slices that overlap enough <code>Overlap</code> method (faster). </li> <li><code>threshold overlap</code>: for the <code>overlap</code> method, associate two nuclei from consecutive slices as one if they overlap by at least more % than the threshold.</li> <li><code>association distance limit micron</code>: For the <code>Munkres</code> method, can associate nuclei from consecutive slices only if they are closer than the distance limit (in microns). </li> </ul>"},{"location":"Get-nuclei/#cellpose","title":"CellPose","text":"<p>CellPose segments the objects in 2D in the xy, yz and xz directions and reconstruct the 3D objects.</p> <p>CellPose uses an isotropic image for the segmentation (same scaling in x, y, and z), so if your image is not isotropic (usually) it will resize it as a first step based on the image scaling parameters. However, results might be better if the isotropic rescaling is done before CellPose, so you might consider doing the rescaling as a preprocessing step.</p> CellPose parameters <ul> <li><code>cell diameter</code>: CellPose needs to know the average size of a nuclei (in pixels). This parameter is the average diameter of a nuclei, in pixels, and will be used to rescale the image to the corresponding size (it needs the average nuclei diameter to be around 18 pixels).</li> <li><code>detection threshold</code>: varies between -6 and 6. Pixels within objects are detected with a detection probability and will be removed if below the threshold. Decrease it to keep more/bigger objects</li> <li><code>resample</code>: run CellPose 3D reconstruction in the resized scale: slower but more precise if the nuclei are smaller than 18 pixels diameter, faster but less precise if the nuclei are bigger than 18 pixels.  </li> </ul> <p>When the segmentation is finished, the labelled nuclei will be displayed in napari, and can be manually corrected.</p>"},{"location":"Get-nuclei/#dask-option","title":"Dask option","text":"<p>This option combines CellPose segmentation with the Dask library for parallel computing. This option is inspired from the distributed segmentation script for CellPose. </p> <p>It allows CellPose to run on very large images when the non distributed version will crash due to lack of memory or be much too slow.</p>"},{"location":"Get-nuclei/#note","title":"Note","text":"<p>Since May 2025, the latest version of CellPose is based on CellPoseSAM. Some parameters are no longer relevant with this version, and it might not be optimal for nuclei segmentation (it is more specialized for cell segmentation). Additionally, this version is much slower to run, especially if you don't have GPU. </p> <p>To use the previous CellPose version with the <code>nuclei</code> trained model (more specialized), you must install it in your Python environment instead of the latest version. In a terminal, or in the napari Terminal interface ( icon at the bottom left of napari window), type: <pre><code>pip install cellpose[distributed]==3.0\n</code></pre> (or select the version 3 or less in the Anaconda interface).</p>"},{"location":"Get-nuclei/#nuclei-editing","title":"Nuclei editing","text":""},{"location":"Get-nuclei/#automatic-correction","title":"Automatic correction","text":"<p>You can remove all nuclei that are smaller than a given volume, or detected only in very few consecutive slices, with the panel <code>Filtering</code> that appears when the segmentation is finished.</p> <p>To filter, check the <code>remove small nuclei</code> option, and choose a threshold volume below which nuclei will be considered as segmentation errors and not kept with the <code>minimum volume</code> parameter.  To remove nuclei that are not detected in several slices, fix the parameter <code>keep ifatleast z</code> to the minimum of Z slices in which one nuclei should be present to not be an error.</p> <p>Click on <code>Update nuclei</code> to perform the automatic filtering and get rid of \"too small\" nuclei. You can close this panel when you don't want to use it anymore.</p>"},{"location":"Get-nuclei/#nuclei-manual-correction","title":"Nuclei: manual correction","text":"<p>Here we also use a label layer to edit the nuclei segmentation. See napari Label layer documentation for more information on the label edition tools available by default in napari (and accessible in the top left panel of the interface)</p> Shortcut/options VisualizationLabel (nuclei) editing F1,F2,F3... Show/hide the 1st,2nd,3rd.. layer (ordered as visibile in the bottom left panel) (bottom left) Switch between 2D/3D view Ctrl-v In 3D view, (de-)activate vispy visualization mode. It allows to set the view perspective, by right-cliking and holding it. Note that in vispy active mode, selecting a label doesn't always work (click coordinates are unprecised) <code>show selected</code> (top left panel) Show only the current label (pixels which have the value taht is currently active the label field) (bottom left panel) Show/hide the corresponding layer v Show/hide the current layer l, <code>show cellnames</code> (right panel) Show/hide the label of the nuclei (in 2D only) <code>ndim=2</code> (top left panel) Modifying a label affects only the current z-slice (2D) <code>ndim=3</code> (top left panel) Modification in one z-slice will be propagated to its neighboring slices 2 Switch to drawing mode (). Draw the current label under the clicks. The precision of the drawing brush is controled by <code>brush stroke</code> parameter (top left panel) 3 Switch to fill mode (). Replace a whole label (clicked nuclei) by the new value (active label). 4 Switch to picking mode () to select a label. When you click on a label (nuclei), it will set the active label to its value (in <code>label</code> field). Ctrl+ Remove the label (nuclei) under the click m Set the active label to the maximum label + 1 (to be sure to create a new nuclei) <code>relabel</code> Renumber all the nuclei from 2 to the number of nuclei. <p>As for junctions, the labels should be unique for each nuclei.  Thus if you want to add a new nuclei don't forget to get the max label + 1 by pressing m. </p> <p>You can see the value of the label of a nucleus by selecting it with the picker tool, or by hovering the mouse pointer on top of it. It displays the value of the current layer intensity below your pointer in the left bottom panel of the napari window, see image below:</p> <p></p> <p>If you are in vispy active mode, the value below your pointer is \"0\" which means that in this view mode, the reading of the position of the pointer is not working well.</p>"},{"location":"Get-nuclei/#saveload-nuclei-file","title":"Save/load nuclei file","text":"<p>When you click on <code>save nuclei</code>, it will save the current nuclei segmentation as a labelled image in the file called imagename<code>nuclei.tif</code> in the results folder. You can load this file to re-edit it later or load the nuclei with the <code>Load segmented file</code> option in the <code>Get nuclei</code> interface. </p> <p>When you have finished the nuclei segmentation, click on <code>Nuclei done</code> to quit this step of the pipeline and go back to the main pipeline.</p>"},{"location":"Hierarchical-clustering/","title":"Hierarchical clustering","text":"<p>Performs hierarchical clustering of the segmented cells from FishFeats results </p> <p>click on <code>Plugins&gt;fishfeats&gt;Hierarchical clustering</code> to start this option</p> <p>From a data table where rows are the segmented cells and columns are measures from FishFeats (e.g. RNA counts, cell area..), it displays the resulting clustering on the segmented cells, colored by cluster.</p>"},{"location":"Hierarchical-clustering/#requirements","title":"Requirements","text":"<p>First, this analysis uses the segmentation of the cells, in 2D, as can be saved from <code>fishfeats</code> main pipeline in the Get cells step.</p> <p>Second, it requires a table file (.csv) that contains the measures to be clustered, as can be saved in the <code>_results.csv</code> file from <code>fishfeats</code> main pipeline in the Get RNAs or Measure cytoplasmic staining steps.</p>"},{"location":"Hierarchical-clustering/#usage","title":"Usage","text":"<p>When you click on <code>Plugins&gt;fishfeats&gt;Hierarchical clustering</code>, the plugin will ask you to choose the image that you are analysing, as it is done in <code>fishfeats</code> main pipeline. Then the interface let you check and update the scale of the image (not important here) and the file that contains the segmentation of the cells in 2D. By default, it will propose the file in the <code>results</code> folder named <code>*imagename*_cells2D.tif</code> if it exists.</p> <p>When you click on <code>Update</code> you get to the main part of the plugin.  First, you have to choose the <code>.csv</code> table file that contains your features to use for the clustering. The file must contain one column named <code>CellLabel</code> that indicates the corresponding cell in the segmentation file. This column does not need to be selected in the features to use for clustering, it must only be present in the file. Each row should contain the feature values of the corresponding cell. </p> <p>When you have selected the file, the plugin will automatically load the names of the columns and show them in the <code>use column</code> parameter. Select the columns (features) you want to use to perform the cell clustering. The unselected columns will not be used. Click on <code>Cluster from selected columns</code> when you have selected the columns of interest. The program will calculate the clustering based on the Ward hierarchical clustering algorithm, then show you the resulting clustering on the segmented cells (one color = one cluster) with the corresponding dendrogram in the right side.</p> <p></p> <p>You can vary the number of clusters to create with the <code>nb clusters</code> parameter. As soon as you change its value, it will update the display. If you want to change the columns used for the clustering, you can still change your selection in the <code>use column</code> parameter but you have to click on <code>Cluster from selected columns</code> again.</p> <p>Finally, you can save the resulting images with the two buttons <code>save clustered cells</code>and <code>save dendrogram</code>. The images will be saved in the <code>results</code> folder, and named <code>*imagename*_ClusteredCells_nclus_*n*.png</code> or <code>*imagename*_ClusteredDendrogram_nclus_*n*.png</code>, with n being the current number of clusters.</p>"},{"location":"Image-scalings/","title":"Image Scalings","text":"<p>Set the image properties (scaling informations, color channels)</p> <p>Choose the option <code>Image scalings</code> in <code>fishfeats</code> main step choices interface to do this step. (1) </p> <ol> <li>This step is loaded by default when you open the image on which to work on.</li> </ol> <p></p> <p>On the right panel, you have a parameter dialog called <code>Scale</code> on which you can set-up the image global parameters:</p> <ul> <li> <p><code>scaleXY</code>: the size in \u00b5m of 1 voxel in the XY direction.</p> </li> <li> <p><code>scaleZ</code>: the size in \u00b5m of 1 voxel in the Z direction.</p> </li> <li> <p><code>direction</code>: direction of imaging- if junctions are above the nuclei when you move in Z, then select <code>top high z</code>, otherwise if the junctions are towards the smaller z, select <code>top low z</code></p> </li> <li> <p><code>junction channel</code>: the number of the color channel that contains the junction staining. The number is indicated in the name of each layers to the left panel: <code>originalChannel0</code>, <code>originalChannel1</code>...</p> </li> <li> <p><code>nuclei channel</code>: the number of the color channel image that contains the nuclei staining. </p> </li> </ul> <p>Channel separation</p> <p>If the nuclei staining is in the same channel as the junction staining, put the same values for the two parameters and the program will separate the two signals later on. If not, be sure to put different numbers, otherwise it will try to separate the two stainings.</p> <p>The plugin reads the metadata of your image and will prefill the scaling parameters based on that. However, there can be some mistakes depending on the metadata format, so it's important to always check that the scaling is correct.</p> <p> A configuration file is created in the <code>results</code> folder when you launch the pipeline. It records some information on your current step and the parameters that you selected. If you quit the plugin with the option <code>Quit plugin</code> it will then save this file that can be read the next time you use <code>fishfeats</code> on the same image. It will then pre-select the parameters to the ones you filled in the first time. The recorded file is saved in the <code>results</code> folder, named as the image but with a <code>.cfg</code> extension.</p> <p>Click <code>Update</code> when you have selected all the parameters to choose the next analysis step to perform.</p>"},{"location":"Installation/","title":"Installation","text":"<p>FishFeats is distributed in a pip module and can be installed as: <pre><code>pip install fishfeats\n</code></pre> in a virtual environment (python 3.10 recommended).</p> <p>If you are not familiar with the procedure, here are two options for installation with more details:</p> <p>Standard installation: You can install napari and <code>FishFeats</code> by creating/using a python virtual environment (recommended).</p> <p>Easier installation: If you have no python virtual environment experience and want to avoid it, you can install napari through the \"bundle\" distribution.  The bundles come with an installer program for Windows and MacOS systemso this allow for an easy installation (everything will be done through graphical interfaces), but allows for less control/flexibility. </p> <p>The table below summarizes the installation options that you have and their compatibility (in November 2025).  For more details for each step, goes to the corresponding paragraph: Standard installation for a python management package based installation, and Easier installation for the bundle app based installation.</p> <p></p>"},{"location":"Installation/#standard-installation","title":"Standard installation","text":"<p>To install FishFeat you need to install napari first.  Napari is Python based and you need to install Python management package if you don't have one already.  You first need to install that (few minutes to 10min), then create a virtual environment and install napari and fishfeats (less than 5 min). </p>"},{"location":"Installation/#from-napari-interface","title":"From napari interface","text":"<p>FishFeats is a napari plugin, in python. You can install it either through an already installed napari by going in napari to <code>Plugins&gt;Install/Uninstall</code>, search for <code>FishFeats</code> and click <code>Install</code>. You could have version issues between the different modules installed in your environment and FishFeats dependencies, in which case it is recommended to create a new virtual environment specific for FishFeats.</p>"},{"location":"Installation/#from-a-virtual-environment-recommended","title":"From a virtual environment (recommended)","text":"<p>Step 1 : install a Python management package (if you don't have one already).  You have different options, but you have to choose one:  </p> <ul> <li>Miniforge (recommended): fast, free but no interface. Follow the detailled steps here to install miniforge</li> <li>venv: fast and simple install in general, no interface. See tutorial here</li> <li>Mamba: fast, free but no interface. See here</li> <li>Anaconda: a sort of interface, works well on MacOS and Windows but slow and might not stay free to all. See here: on windows, on macOS or on linux ). </li> </ul> <p>Step 2 : Create a virtual environment with that python management package.  Once you have installed such Python management package, they generally create one environment (=call env) called base but you should not install anything in that env.  </p> <ul> <li> <p>You need to create an environment where you will install FishFeats and napari. To do so you first open the terminal, then you can write <code>conda env list</code> to list all your environments.  If you don\u2019t already have one env for fishfeats you can create one by : <code>conda create -n fishfeats_env python=3.10</code>  It will install python 3.10 and create an environment called <code>fishfeats_env</code>. </p> </li> <li> <p>Then you need to activate that environment (=like entering or opening it) by writing : <code>conda activate fishfeats_env</code> </p> </li> <li> <p>Then you can install napari and FishFeats and all the depencies they need : <code>pip install fishfeats</code>.  It takes a bit of time, be patient. Congrats and see you in Step 3 then. </p> </li> </ul> <p>Step 3: Open napari You can open napari by writing <code>napari</code> in the terminal.  It is often slow to open the first time but that\u2019s it. </p> <p>Extra information: </p> <ul> <li> <p>you can create more environment, they are rather light in size. Be careful because with Python, and napari of course, plugin and librairies are often dependent on many other to properly works and very often, installing a new one can create major issues in the environment that was previously working.... </p> </li> <li> <p>to logout, you can just close the terminal. To come back to FishFeats, you can open the terminal, enter <code>conda activate fishfeats_env</code>, <code>napari</code> </p> </li> </ul> <p>Installation of dependencies</p> <p>FishFeats depends on a lot of external librairies, some of which are under active development and others that are no longer maintained. We tried to put few constraints on the librairy versions to allow for flexibility in the installation, but also to identify which ones should be limited. Don't hesitate to look at our Trouble shooting page, where we listed some environment configurations that worked and could thus be reproduced.</p>"},{"location":"Installation/#update-fishfeats","title":"Update FishFeats","text":"<p>To get the latest version of FishFeats when it is updated, type <pre><code>pip install -U fishfeats\n</code></pre> in your activated environment.  If FishFeats was updated since you last installed/updated it, the latest version will be downloaded.</p>"},{"location":"Installation/#start-fishfeats","title":"Start FishFeats","text":"<p>Open napari by typing <pre><code>napari\n</code></pre> in the activated environnement and goes to <code>Plugins&gt;FishFeats&gt;Start fishfeats</code></p>"},{"location":"Installation/#compatibilitydependencies","title":"Compatibility/Dependencies","text":"<p><code>FishFeats</code> depends on several Python modules to allow different tasks. It is not necessary to install all the dependencies to run it, only the ones listed in <code>setup.cfg</code> configuration file. When installing the plugin, the listed dependencies will be automatically installed. </p> <p>Other dependencies can be installed individually if the corresponding option will be used (e.g. install cellpose: <code>pip install cellpose</code>). They can also be all installed by installing FishFeats in full mode <code>pip install</code>fishfeats[full]<code></code>.</p>"},{"location":"Installation/#operating-system","title":"Operating System","text":"<p>The plugin has been developped on a Linux environment and is used on Windows and MacOS distributions. It should thus be compatible with all these OS provided to have the adequate python environments.</p> Windows GPU drivers <p>We encountered an unsolved yet error only on Windows with some specific nvidia drivers/GPU card. During plugin usage, it returns this error:<code>OSError: exception: access violation reading 0x0000000000000034</code>. See here for more infos.</p>"},{"location":"Installation/#python-version","title":"Python version","text":"<p>We tested the plugin with python 3.9, 3.10, 3.11 with napari 0.4.19, 0.6.1.  In Trouble shooting, we listed some environments that worked for given operating system/Python version.  You can also create your environment directly from these <code>.yaml</code> files.</p> <p>There is an incompability with napari 0.4.17 (strongly not recommended) for point edition in 3D.</p> <p>Please refers to Trouble shooting if you encounter issues at the installation/usage or to the repository issues. Finally if you don't find any information on your error, open a new issue in this repository.</p>"},{"location":"Installation/#full-working-configuration","title":"Full working configuration","text":"<p>We listed examples of fully working configuration in <code>Windows</code>, <code>MacOS</code> and <code>Ubuntu</code> operating systems in the Trouble shooting page. You can compare the version of the dependencies to the ones in your environment in case of issue.</p>"},{"location":"Installation/#easier-installation","title":"Easier installation","text":""},{"location":"Installation/#napari-installation-through-a-graphical-interface","title":"napari installation through a graphical interface","text":"<p>Download the bundle version of napari 0.5.4:</p> <ul> <li>Linux version</li> <li> <p>Windows x86</p> </li> <li> <p>MacOS arm64 (apple chip, from M1 and after) </p> </li> <li>MacOS x86 (Intel chip, before M1)</li> </ul> <p>napari bundle doesn't open on MacOS</p> <p>On MacOS, the bundle version 0.5.4 might not work (after installation, it doesn't open, see more in imagesc forum). In this case, proceed to the \"normal\" installation, or install the latest napari bundle (without epyseg then)). </p> <p>Double-click on the executable. It will open an installer program, that you can simply follow step by step. You can keep all the default options that are proposed by the installation program. When the installation is finished (it takes some time), a shortcut icon should have been created in your desktop.</p> <p></p> <p>All these bundles come from napari github, release of version 0.5.4.  The installation steps for each OS are described here.</p> Why napari version 0.5.4 <p>We chose this version as it is the last one in Python 3.9, the following ones are with Python &gt; 3.11, to have all FishFeats options available (including Epyseg which is limited to version &lt;3.11). You can still download the latest bundle of napari if you wish, and then use the napari console Terminal to fix dependencies install, or not use some options (Epyseg, and eventually Stardist and SepaNet which are based on Tensorflow).</p>"},{"location":"Installation/#fishfeats-installation-through-a-graphical-interface","title":"FishFeats installation through a graphical interface","text":"<p>When the installation is over, double-click on the napari icon and wait for napari window to open (it can take a few minutes).  When napari is open, go to <code>Plugins&gt;Install/Uninstall</code> to open the plugin manager. In the window that appears, search for <code>fishfeats</code> and click Install. Wait for the installation to finish (it takes some time), and restart napari. You can now use FishFeats by going in <code>Plugins&gt;fishfeats&gt;start fishfeats</code>. </p> <p></p> <p>If you want to install a specific version of FishFeats, click on <code>Installation info</code> to get the list of available versions. Restart napari after the plugin installation.</p> Updating some dependencies version <p>If you need to change some dependencies version, you can do so by opening the napari Terminal by clicking the icon  at the bottom left of the napari window. Then write <code>pip install modulename==versionnumber</code> to install the <code>modulename</code> library with the given version number.</p>"},{"location":"Known-errors-and-solutions/","title":"Troubleshooting","text":""},{"location":"Known-errors-and-solutions/#encountered-errors-and-solutions","title":"Encountered errors and solutions:","text":""},{"location":"Known-errors-and-solutions/#module-versions","title":"Module versions","text":"<ul> <li><code>skimage.morphology.selem</code> module not found error =&gt; Problem of compatibility between Big-fish and scikit-image versions. Ideally, chooses a recent version of skimage (scikit-image==0.19.3) and Big-fish (big-fish==0.6.2).</li> <li><code>numpy.core.multiarray failed to import</code> (on python 3.11, when calling stardist) =&gt; Problem of compatibility between Numpy version and Stardist. You can downgrade Numpy to 1.26, Tensorflow to 2.14.</li> <li><code>DNN library is not found.      [[{{node model/down_level_0_no_0/Relu}}]] [Op:__inference_predict_function_14500]</code> (on python 3.10, when calling stardist, epyseg or SepaNet) =&gt; Problem of Tensorflow version compatibility with your cuda drivers installation. See here for the compatible versions cuda-tensorflow. See also this discussion about this problem.</li> </ul>"},{"location":"Known-errors-and-solutions/#weird-point-selection","title":"Weird point selection","text":"<p>We encountered a weird point selection on a version of napari, where the selected points did not correspond at all to the drawn selection rectangle. This is due to napari version 0.4.19, we strongly recommend to avoid this version.</p>"},{"location":"Known-errors-and-solutions/#acces-violation-reading","title":"Acces violation reading","text":"<p><code>OSError: exception: access violation reading 0x0000000000000034</code>.</p> <p>This error happened only in Windows with specific nvidia card (A6000). It happens when adding or deleting Shape layers, quite often in <code>cytoplasmic measure</code> option. It seems to be an error external to the plugin or napari.  We haven't found a solution yet, but please refer to this discussion on imagesc forum for more info/updates.</p>"},{"location":"Known-errors-and-solutions/#other-issues","title":"Other issues","text":"<p>You can also check on the issues page of the repository to see if your problem has already been reported and has a solution.  Otherwise, open a new one in this page and we will do our best to answer fast.</p>"},{"location":"Known-errors-and-solutions/#tested-and-working-configurations","title":"Tested and working configurations","text":"<p>Here we proposed the list of versions that worked fine for us. For some versions, we provide the full list of package versions that were installed on several python environments, with the corresponding operating system, that worked fine for us.</p> <p>For each set-up, we list first the graphical info that we get with <code>napari --info</code>, then the link to the full yaml file.</p> <p>Note that <code>Epyseg</code> cannot be install on python versions above 3.10.  Thus, to use the full pipeline with all options, we recommend python 3.10.  However, if you don't intend to use Epyseg or use it separately, the pipeline and the other dependencies are compatible with more recent python versions.</p> Environment lists WindowsMacOSLinux <p> Windows 10, python 3.9.21, napari 0.4.18  <pre><code>fishfeats: 1.1.11               \nnapari: 0.4.18\nPlatform: Windows-10-10.0.19045-SP0\nPython: 3.9.21 | packaged by conda-forge | [MSC v.1929 64 bit (AMD64)]\nQt: 5.15.2\nPyQt5: 5.15.11\nNumPy: 1.26.4\nSciPy: 1.13.1\nDask: 2024.8.0\nVisPy: 0.12.2\nmagicgui: 0.9.1\nsuperqt: 0.6.7\nin-n-out: 0.2.1\napp-model: 0.2.8\nnpe2: 0.7.7\n\nOpenGL:\n     - GL version:  4.6.0 NVIDIA 571.59\n     - MAX_TEXTURE_SIZE: 32768\n</code></pre> <p>yaml file with all python packages installed in the environment here </p> <p> Windows 10, python 3.10.18, napari 0.6.1  <pre><code>napari: 0.6.1\nPlatform: Windows-10-10.0.19045-SP0\nPython: 3.10.18 | packaged by conda-forge | MSC v.1943 64 bit (AMD64)\nQt: 5.15.2\nPyQt5: 5.15.11\nNumPy: 1.26.4\nSciPy: 1.15.3\nDask: 2025.5.1\nVisPy: 0.15.2\nmagicgui: 0.10.1\nsuperqt: 0.7.5\nin-n-out: 0.2.1\napp-model: 0.3.2\npsygnal: 0.13.0\nnpe2: 0.7.8\npydantic: 2.11.7\n\nOpenGL:\n     - PyOpenGL: 3.1.9\n     - GL version:  4.6.0 NVIDIA 571.59\n     - MAX_TEXTURE_SIZE: 32768\n     - GL_MAX_3D_TEXTURE_SIZE: 16384\n\nOptional:\n      - numba: 0.61.2\n      - triangle: 20250106\n      - napari-plugin-manager: 0.1.6\n      - bermuda: 0.1.4\n      - PartSegCore not installed\n\nExperimental Settings:\n      - Async: False\n      - Autoswap buffers: False\n      - Triangulation backend: Fastest available\n\nfishfeats: 1.1.11\n</code></pre> <p>yaml file with all python packages installed in the environment here </p> <p> MacBook pro M1, python 3.10.14, napari 0.4.19  <pre><code>napari: 0.4.19\nPlatform: macOS-15.5-arm64-arm-64bit\nSystem: MacOS 15.5\nPython: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:51:49) [Clang 16.0.6 ]\nQt: 5.15.8\nPyQt5: 5.15.9\nNumPy: 1.26.4\nSciPy: 1.13.1\nDask: 2025.5.1\nVisPy: 0.14.3\nmagicgui: 0.10.1\nsuperqt: 0.7.5\nin-n-out: 0.2.1\napp-model: 0.2.8\nnpe2: 0.7.8\n\nOpenGL:\nGL version:\u2006 \u2006 2.1 Metal - 89.4\nMAX_TEXTURE_SIZE: 16384\n\nfishfeats: 1.1.11\n</code></pre> <p>yaml file with all python packages installed in the environment here </p> <p> MacBook pro M1, python 3.10.14, napari 0.5.5  </p> <p> MacBook pro M1, python 3.10.14, napari 0.6.2  </p> <p> Ubuntu 20.04.6, python 3.10.0, napari 0.6.1  <pre><code>napari: 0.6.1\nPlatform: Linux-5.15.0-139-generic-x86_64-with-glibc2.31\nSystem: Ubuntu 20.04.6 LTS\nPython: 3.10.0 | packaged by conda-forge | (default, Nov 20 2021, 02:24:10) [GCC 9.4.0]\nQt: 5.15.2\nPySide2: 5.15.2.1\nNumPy: 1.24.2\nSciPy: 1.15.3\nDask: 2025.5.1\nVisPy: 0.15.2\nmagicgui: 0.10.0\nsuperqt: 0.7.3\nin-n-out: 0.2.1\napp-model: 0.3.1\npsygnal: 0.13.0\nnpe2: 0.7.8\npydantic: 2.11.5\n\nOpenGL:\n    - PyOpenGL: 3.1.9\n    - GL version:  4.6.0 NVIDIA 545.29.06\n    - MAX_TEXTURE_SIZE: 32768\n    - GL_MAX_3D_TEXTURE_SIZE: 16384\n\nOptional:\n- numba: 0.61.2\n- triangle not installed\n- napari-plugin-manager not installed\n- bermuda not installed\n- PartSegCore not installed\n\nExperimental Settings:\n- Async: False\n- Autoswap buffers: False\n- Triangulation backend: Fastest available\n\nfishfeats: 1.1.3\n</code></pre> <p>yaml file with all python packages installed in the environment here </p> <p> Ubuntu 20.04.6, python 3.11, napari 0.6.2 - No EPYSEG  <pre><code>napari: 0.6.2\nPlatform: Linux-5.15.0-139-generic-x86_64-with-glibc2.31\nSystem: Ubuntu 20.04.6 LTS\nPython: 3.11.13 | packaged by conda-forge | (main, Jun  4 2025, 14:48:23) [GCC 13.3.0]\nQt: 5.15.14\nPyQt5: 5.15.11\nNumPy: 1.26.0\nSciPy: 1.16.0\nDask: 2025.7.0\nVisPy: 0.15.2\nmagicgui: 0.10.1\nsuperqt: 0.7.5\nin-n-out: 0.2.1\napp-model: 0.4.0\npsygnal: 0.14.0\nnpe2: 0.7.9\npydantic: 2.11.7\n\nOpenGL:\n    - PyOpenGL: 3.1.9\n    - GL version:  4.6.0 NVIDIA 545.29.06\n    - MAX_TEXTURE_SIZE: 32768\n    - GL_MAX_3D_TEXTURE_SIZE: 16384\n\nOptional:\n - numba: 0.61.2\n - triangle not installed\n - napari-plugin-manager not installed\n - bermuda not installed\n - PartSegCore not installed\n\nExperimental Settings:\n  - Async: False\n  - Autoswap buffers: False\n  - Triangulation backend: Fastest available\n\nfishfeats: 1.1.15\n</code></pre> <p>yaml file with all python packages installed in the environment here</p> <p> </p> <p> Ubuntu 20.04.6, python 3.9, napari 0.4.19  <pre><code>napari: 0.4.19\nPlatform: Linux-5.15.0-139-generic-x86_64-with-glibc2.31\nSystem: Ubuntu 20.04.6 LTS\nPython: 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:50:21) [GCC 12.3.0]\nQt: 5.15.2\nPySide2: 5.15.2.1\nNumPy: 1.24.3\nSciPy: 1.13.0\nDask: 2024.8.0\nVisPy: 0.14.3\nmagicgui: 0.10.0\nsuperqt: 0.7.3\nin-n-out: 0.2.1\napp-model: 0.2.8\nnpe2: 0.7.8\n\nOpenGL:\n- GL version: 4.6.0 NVIDIA 545.29.06\n- MAX_TEXTURE_SIZE: 32768\n\nfishfeats: 1.1.31\n</code></pre> <p>yaml file with all python packages installed in the environment here </p>"},{"location":"Measure-cytoplasmic-staining/","title":"Measure cytoplasmic intensity","text":"<p>Measure the intensity in one or more channels in the cytoplasm of cells</p> <p>To measure cytoplasmic intensity, choose the Measures:Cytoplasmic intensity in the main pipeline interface.</p> <p>See the typical workflow of this step here</p> <p>This step allows you to measure the average intensity (normalised or not) in the cells (after segmentation) in a few Z slices close to the cell junctions. You can measure the intensity in several channels. By default, one is initally proposed: click on the + sign next to the <code>cyto chanels</code> parameter to increase the number of channels to measure (or - to remove one). Then, for each channel, set the number to the corresponding <code>originalChanel*</code> staining that you want to measure with the <code>cyto chanels</code> parameter(s). When you change the value of this parameter, the plugin shows you directly which chanel you are measuring. </p> <p></p> <p>The parameter <code>z_thickness</code>controls the number of slices below the cell surface that will be used in the measurement. In the image below, the intensity will be averaged on 3 slices starting from each cell surface.</p>"},{"location":"Measure-cytoplasmic-staining/#background-rectangle","title":"Background rectangle","text":"<p>When you choose a chanel or add a new one, a layer <code>backgroundRectangle*</code> appears in the left panel. The number at the end of the name corresponds to the number of the channel to analyse. This rectangle will be used to normalise the intensity of this channel by dividing by its mean intensity, therefore you must place this rectangle in a background area of your staining, and in the typical Z-slice where you are going to measure the signal (should be there by default). You can move it or change its size. </p>"},{"location":"Measure-cytoplasmic-staining/#output","title":"Output","text":"<p>You can set the parameters to either pop-up the measurement table, add a new layer with the cells filled by the measured intensities or save the measurement table to an excel file. The image of the measurement intensities shows the normalised intensity measured in each cell, in the same color as the original chanel, ranging from dark for low values of normalised intensity to bright for high intensities. There will be one image for each measurement channel.</p> <p></p> <p>The measurement table gives you a list of all cells with their label and position. The last columns are the measured intensity in each measurement channel. For each channel, 4 values are given: the mean intensity, its standard deviation, the normalised mean intensity and its standard deviation. The columns are named <code>Cyto*_MeanIntensity</code>... where the number after <code>Cyto</code> is the number of the measured channel (staining).</p>"},{"location":"Measure-nuclear-staining/","title":"Measure nuclear intensity","text":"<p>Measure the intensity in the nuclei of cells across one or more channels</p> <p>To measure nuclear intensity, choose the option Measure: nuclear intensity in the main pipeline interface</p> <p>See the typical workflow of this step here</p> <p>To measure signal intensity in the nuclei, they must have been segmented before.  If you haven't done it yet, go to nuclei segmentation step first.</p> <p>This step allows you to measure the nuclear intensity of a chosen staining inside each segmented nucleus. Choose the channel to measure and click on <code>Measure</code> to launch the computation. The measure can take time.</p> <p></p> <p>Once the measure is done, you can visualize the table of measurements by clicking <code>Show measures table</code>. A new window will pop up with the list of nuclei and their label, along with the measures in the selected channel.</p> <p>Clicking <code>Save and stop</code> will save these results for all nuclei associated to one cell in the imagename_results.csv result file. However, if you want to save the nuclear intensity in ALL nuclei (not only the ones that are linked to a cell), you can click on <code>Save all nuclei measures</code>. This will create a new result file in the <code>results</code> folder, called imagename_nuclei.csv that contains the displayed table data.</p>"},{"location":"Measures/","title":"Measures","text":"<p>Fish&amp;Feats offers several options to quantify tissue properties at the single cell level:</p> <ul> <li>Classify cells: classify cells into user-defined categories</li> <li>Measure cytoplasmic intensity: Measure the intensity of selected staining(s) in each cell cytoplasm </li> <li>Measure nuclear intensity: Measure the intensity of selected staining(s) in each cell nucleus or all nuclei </li> <li>Measure RNA intensity: Measure the intensity of selected staining(s) in each segmented RNA </li> </ul> <p>See the typical workflow of the measurement steps here</p>"},{"location":"Open-image/","title":"Open image","text":"<p>Open the raw image to analyse</p>"},{"location":"Open-image/#load-raw-images-from-fishfeats","title":"Load raw images from FishFeats","text":"<p>FishFeats can directly load raw images with format <code>.tif, .ims or .czi</code> and read their metadata.</p> <p>If your raw images have one of these formats, start FishFeats \"normally\" by going to <code>Plugins&gt;fishfeats&gt;Start</code> and select your image. Then you can proceed with the main pipeline, by first checking and updating the image metadata information (scale, channels)</p>"},{"location":"Open-image/#load-other-format-images","title":"Load other format images","text":"<p>If your raw images have another format not handled by FishFeats, you have to open them first in napari. Open the image with napari built-in readers or other plugin readers that are compatible with your image format. For example, this plugin offers options to open a lot of common microcospy data format.</p> <p>The image can be opened as a single layer in napari (containing both the channels and the 3D dimensions) or as several layers, one by color channel.  However, the only opened layers should be the one of the image to analyse.</p> <p></p> <p>Then select <code>Plugins&gt;fishfeats&gt;Start from layer(s)</code> to start FishFeats with the open layer(s).  You get an extra step, asking you to select the full path to the original opened image.  This is necessary to correctly set the image name and path to save the results/configuration in the <code>results</code> folder in the same folder, or load previous results if there are any.</p> <p></p> <p>Click on <code>Set path</code> when you have selected the correct image and the plugin will then goes to the main pipeline step of checking/setting the image metadata.</p>"},{"location":"Preprocess-nuclei/","title":"Preprocess nuclei","text":"<p>Preprocess the nuclei channel for better segmentation</p> <p>To preprocess the nuclei staining image, choose the option Nuclei:Preprocess in the main pipeline interface.</p> <p>You can either apply filters to improve the image, or use noise2void, a deep learning method that reduces the noise in images.</p> <p>Applying preprocessing outside FishFeats</p> <p>If you want to apply other preprocessing that are not yet available in FishFeats, you can either work with another software with the required softwares and save the preprocessed nuclei in the input image, or work other napari plugins and use the option to start FishFeats from already opened layers.  You can also contact us (filling an issue in this repository) to ask for the possibility to add a given preprocessing algorithm in FishFeats</p> <p></p>"},{"location":"Preprocess-nuclei/#filtering","title":"Filtering","text":"<p>FishFeats proposes several classic filters to improve the image quality for the segmentation task. In some cases, this can help the segmentation process.</p> <p>Median filter: If you have a lot of small noisy dots, you can apply a median filter that will locally smooth the image and get rid of small bright or dark dots. Try to take a filter radius below the average size of a nuclei. The filter is much slower for higher radii.  Select the option <code>Median filtering</code> and click on <code>Apply preprocessing</code>.</p> <p>Remove background: if you have an heterogeneous background in the image (eg a darker corner) and that it seems to impair the performance of the segmentation algorithm (it's not always the case if local normalization is applied), you can remove the background to have more homogeneous local intensities, and a background always close to 0.  Select the option <code>Remove background</code> and click on <code>Apply preprocessing</code>.</p> <p>If you are happy with the filtering, click on <code>Preprocessing done</code> to use this new version in the segmentation process. Otherwise, click on <code>Reset nuclei staining</code> to test other parameters.</p>"},{"location":"Preprocess-nuclei/#noise2void","title":"Noise2Void","text":"<p>Denoising with noise2void requires the napari plugin <code>napari-n2v</code> that you can install in the virtual environment: <code>pip install napari-n2v</code> or through the napari plugin interface.</p> <p>Click the button <code>Noise2Void</code> to open the napari noise2void plugin interface. It will open an interface to perform denoising with a trained noise2void model.</p> <p>If you don't have a trained noise2void model, you have to train one through the noise2void plugin, externally of FishFeats.  Noise2void doesn't need annotated data to train, so you will only have to load your image in it and use the options to train it. </p>"},{"location":"Separate-junctions-and-nuclei/","title":"Separate stainings","text":"<p>Separate junctions and nuclei staining acquired in the same chanel</p> <p>Choose <code>separate junctions and nuclei</code> in the main pipeline interface to do this step and change the separation parameters. (1) </p> <ol> <li>If you do directly the junction or nuclei segmentation, the plugin will automatically do this step with the default parameters. When the separation is not good, then it is necessary to follow this step to choose better parameters.</li> </ol> <p>When the <code>junction channel</code> and <code>nuclei channel</code> parameters from the Image scalings interface are set to the same value, the program will try to separate automatically the two stainings to have one only-junction signal and one only-nuclei. If this step is not called before, the <code>Cells:Segment</code> and <code>Nuclei:Segment</code> steps will call it. </p> <p>\u26a0\ufe0f The separated images are artificial. Be careful if you plan to do intensity measurements, the value of the pixels in these separated images are not relevant! The separated images are useful for segmentation purposes, for measurement you should use the original image.</p>"},{"location":"Separate-junctions-and-nuclei/#separation-methods","title":"Separation methods","text":"<p>Three options to separate are proposed:</p> <ul> <li> <p><code>sepaNet</code>: SepaNet uses a neural network trained to separate junctions and nuclei. </p> </li> <li> <p><code>Tophat filter</code>: Tophat uses several filters to separate line structures from large roundish structures.</p> </li> <li> <p><code>Load</code>: if the separated files were previously saved, you can directly load them and avoid doing the separation again. The pipeline proposes the <code>Load</code> option by default when the files are present.</p> </li> </ul> <p></p> <p>When the separation is finished, two new layers will appear, called <code>nucleiStaining</code> in blue and <code>junctionsStaining</code> in red. You can check that the separation worked well enough and re-run it with new parameters if not.  </p>"},{"location":"Separate-junctions-and-nuclei/#sepanet","title":"SepaNet","text":"<p>When using this method, you must give it the location of the trained neural network (SepaNet), by selecting it through the <code>sepanet_path</code> parameter. Go inside the network folder to select it (you should see the <code>assets</code> and <code>variable</code> folders). Then the plugin will directly run the prediction with the selected network.</p> <p>SepaNet has been trained on several images from zebrafish stained for nuclei with PCNA or DAPI and for junctions with ZO1. The two stainings were separated and sometimes we also had the mixed staining acquisition, otherwise there were mixed artificially for the training.</p>"},{"location":"Separate-junctions-and-nuclei/#tophat-filtering","title":"Tophat filtering","text":"<p>With this method, several parameters need to be tuned to your image scaling:</p> <ul> <li> <p><code>tophat radxy</code>: to favor junctions over nuclei (lines compared to full spheres), a top hat filter is applied to the image. This parameter is the size of this filter in the xy direction. It will keep as junctions the structures (lines) that are smaller in width than the xy radius.</p> </li> <li> <p><code>tophat radz</code>: size of the top hat filter in the z direction.</p> </li> <li> <p><code>outlier thres</code>: outlier points are removed before applying the filtering, to not be too influenced by small bright points. Decrease this threshold to remove more small bright points, increase it if it removes too much signal.</p> </li> <li> <p><code>smooth nucleixy</code>: after applying the separation, applies a smooth filter on the nuclei signal, size in xy</p> </li> <li> <p><code>smooth nucleiz</code>: size in z of the smoothing filter for the nuclei signal</p> </li> </ul>"},{"location":"Separate-junctions-and-nuclei/#other-parameters","title":"Other parameters","text":"<ul> <li><code>close_layers</code>: if checked the two created layers <code>nucleiStaining</code> and <code>junctionStaining</code> will be closed at the end of this step, when clicking on <code>Separation done</code>.</li> <li><code>Save separated</code>: save the two separated images in the <code>results</code> folder. This is advised to reload the process later and avoid having to perform the separation again. </li> </ul>"},{"location":"Step-by-step/","title":"Step-by-step example","text":"<p>Here, we propose step-by-step examples of possible usage of the pipeline. You can download test data from this zenodo repository: https://zenodo.org/records/17048217 to follow it.</p> <p>All the examples here are done with the image <code>AB3-HG-AQUCISITION-4CHANNELS-SHRNActrl-filtered_minicrop.ims</code>.</p>"},{"location":"Step-by-step/#a-count-the-number-of-smfish-spots-by-cell","title":"A- Count the number of smFish spots by cell","text":""},{"location":"Step-by-step/#a1-open-the-image-and-checkset-the-metadata","title":"A1 - Open the image and check/set the metadata","text":"<p>Open <code>napari</code> and start FishFeats with <code>Plugins&gt;FISHFEATS&gt;Start fishfeats</code>.</p> <p>A window dialog appears to select the image to analyse.  Browse your folders and select the downloaded image <code>AB3-HG-AQUCISITION-4CHANNELS-SHRNActrl-filtered_minicrop.ims</code> (or another one on which you want to do this test).</p> <p>The image is loaded and is displayed with its color channels side by side.  At the right of the interface, you can set the metadata of the image.</p> <p>Check that the scaling metadata were correctly loaded and correct them if needed (see image below).</p> <p></p> <p>Then, check that the 3D direction of the image is correct (the higher Z slices correspond to the apical cells = <code>top high z</code>).</p> <p>Set the channels number that contains the junction staining (Zo1) and/or the nuclei staining (DAPI, PCNA, Sox2).  In our example image, the first channel contains the junction (Zo1), which correspond to number <code>0 (originalChannel0</code>), in red. The nuclei channel is the last channel, <code>originalChannel3</code>.</p> <p>Finally click on <code>Update</code> to set these properties for the image.  All this set-up will be saved in the configuration file <code>AB3-HG-AQUCISITION-4CHANNELS-SHRNActrl-filtered_minicrop.cfg</code> associated with this image. When you reload this image in FishFeats, it will read this configuration file and reload it.  This file can also be useful if you don't remember which option and parameters you selected for a given step.  You can open this file and you will see the list of steps and parameter values used.</p> <p>You can then choose which steps to perform depending on what you need for your analysis by selecting the step in the <code>Main</code> panel in the right side of the window.</p>"},{"location":"Step-by-step/#a2-segment-epithelial-cells","title":"A2 - Segment epithelial cells","text":"<p>To segment the apical contour of the cells, choose Cells:Segment option.</p> <p>If you have already done this step, the plugin will write <code>Found projection file</code> and/or <code>Found cell file</code> if it found the corresponding files with the default names.  You can then choose the option <code>Load previous files</code> to directly load them and go to the manual curation step.</p> <p>Otherwise, select <code>do projection and segmentation</code>.</p> <p>An interface opens to perform the projection. You can either load a file of the projected junction staining if you have done it with another external software (e.g. CARE, LocalZProjector), by clicking on <code>choose file</code>. </p> <p>Otherwise, click on <code>Project now</code> to calculate the local projection with FishFeats. The projected results will be overlaid in white in your window (image below). If you are not satified with the results, you can click on <code>Advanced</code> to change the parameters and <code>Project now</code> again.</p> <p></p> <p>Then click on <code>Projection done</code> to perform the segmentation on the projected channel.</p> <p>Choose <code>Epyseg</code> in the <code>Method</code> interface to perform cell contour segmentation (or CellPose). If you haven't installed <code>Epyseg</code> (not mandatory), you will get an error message.  Install it in your virtual environment by typing <code>pip install epyseg</code>.</p> <p>Click on <code>segment cells</code> and wait for the segmentation to be calculated (0.105 min on this test image with 1 GPU).</p> <p></p> <p>You can manually correct the segmentation with shortcuts that are indicated at the top left of your window.</p> <p>Click on <code>Cells done</code> to finish this step and save the results. Wait while the program saves the data and replaces the projected cells in 3D. This can take a little time as it is looking for the best Z-position for each cell.</p> <p>You have now segmented all the apical cell contours and estimated their position in 3D. If you open the result file (<code>AB3-HG-AQUCISITION-4CHANNELS-SHRNActrl-filtered_minicrop_results.csv</code>) in a table editor (e.g. Excel, R, Prism..) you can already analyse these data (number of cells, apical area, position, etc.). This file will be enriched with all of the new information that you will add by performing other steps.</p>"},{"location":"Step-by-step/#a3-segment-the-smfish-staining","title":"A3 - Segment the smFish staining","text":"<p>Select Get RNA in the <code>Main</code> interface for this step.</p> <p>The interface that opens allows you to choose the channel to segment, and the parameters for the segmentation. Choose <code>2</code> in <code>RNA channel</code> parameter.</p> <p>You can now segment this channel with Big-fish (if it is not installed (not mandatory), install it with <code>pip install big-fish</code>). Choose the approximate size of one dot in XY and in Z (not the same as one another because with imaging, a circular spot is usually a rugby-balloon shape).  Put <code>1200</code> nm for <code>Spot Z radius</code> and <code>400</code> nm for <code>Spot X,Y radius</code>. Select <code>Automatic</code> for the threshold parameter to let Big-fish select the intensity threshold automatically. If the automatic threshold does not yeild a good result, you can then unclick it and the value determined by Big-fish will be written in the parameter box. You can then modify it.</p> <p></p> <p>Click on <code>Get RNA channel</code> to perform the segmentation with the selected parameters.</p> <p>Wait for the segmentation to be calculated (0.103 min on the test image).</p> <p>When it is finished, a new layer is added to the view, called <code>assignedRNA2</code>.  The center of each RNA spot is displayed as a napari point in white. Two new panels have been added to the right side interface of the <code>RNAs</code> option.  You can change the points size with the interface in the right side, as well as color them by specific properties, in the <code>Point display</code> part in <code>EditRNA2</code> panel.</p> <p></p>"},{"location":"Step-by-step/#a4-assign-the-segmented-dots-to-their-cell-and-count-them","title":"A4 - Assign the segmented dots to their cell and count them","text":"<p><code>Automatic assignment</code> options allow to perform an automatic assignment of each spot to its most likely cell. We will do assignment with the simplest method, which is the <code>Projection</code>. Each spot will be assigned to the apical cell that it belongs to after an orthogonal Z-projection.</p> <p>Select <code>Projection</code> in <code>Assignment method</code> and click on <code>Apply assignment</code>.</p> <p>When the calculation is finished (0.004 min on our test image), the points are displayed with their assigned cell color.  The contour of the cells are also overlaid to see the correspondance.</p> <p></p> <p>You can correct the assignments with the available shortcuts (see text in the top-left part of the window) and with the <code>Point correction</code> panel.</p> <p>You can save the results while you are performing the correction by clicking on <code>Save RNAs</code> and click on <code>Save and quit RNA 2</code> when all is done. e you are performing the correction by clicking on <code>Save RNAs</code> and click on <code>Save and quit RNA 2</code> when all is done.</p> <p>In the results table, you will now have a new column with the counts of RNA 2 in each cell. You can also display directly these counts by clicking on <code>Draw RNA 2 counts</code> in the <code>EditRNA2</code> panel.</p> <p>You obtain the map of the cells, where each cell is colored by its number of RNA2 spots assigned to it by Projection, then manually corrected.</p> <p></p>"},{"location":"Step-by-step/#b-classify-the-cells-by-their-intensity-in-one-channel","title":"B - Classify the cells by their intensity in one channel","text":""},{"location":"Step-by-step/#b1-open-the-image-and-set-metadata","title":"B1 - Open the image and set metadata","text":"<p>Open <code>napari</code> and start FishFeats with <code>Plugins&gt;FISHFEATS&gt;Start fishfeats</code>.</p> <p>A window dialog appears to select the image to analyse.  Browse your folders and select the downloaded image <code>AB3-HG-AQUCISITION-4CHANNELS-SHRNActrl-filtered_minicrop.ims</code> (or another one on which you want to do this test).</p> <p>The image is loaded and is displayed with its color channels side by side.  At the right of the interface, you can set the metadata of the image.</p> <p>If you have performed the steps for the previous example, the metadata are reloaded and already set. Click on <code>Update</code> and directly go the next step.</p> <p>Otherwise, follow the step A1 of the previous example.</p>"},{"location":"Step-by-step/#b2-segment-the-apical-cell-contour","title":"B2 - Segment the apical cell contour","text":"<p>If you have done the previous example A, the cell are already segmented and should be loaded if you let <code>load previous</code> parameter selected. Then, you can directly go to the Measure:Classify cells step.</p> <p>Else, see the step A2 to perform apical cell contour segmentation.</p>"},{"location":"Step-by-step/#b3-classify-the-cells-by-intensity","title":"B3 - Classify the cells by intensity","text":"<p>Select Measure:Classify cells in the <code>Main</code> interface to do this step.</p> <p>It will load the cell contours so that you can classify them. In the interface, select the <code>Add/Edit a feature</code> panel and write the name of the feature you want to classify, e.g. <code>Positive</code>.</p> <p></p> <p>Click on <code>Do feature</code> to open an interface that allows you to initialize the cell classification.</p> <p></p> <p>Select <code>channel projection+threshold</code> to set an initial classification based on the projected intensity of one channel. Select <code>1</code> in <code>channel to project</code> parameter to perform this thresholding on the <code>originalChannel1</code> channel (the green one). Set <code>Threshold</code> to <code>1.2</code> and click <code>Create new feature</code> to launch the classification.</p> <p></p> <p>A new layer called <code>Feat_nameofyourfeatureCells</code> is added to the viewer. It contains the cells colored by their class.  You can easily edit this classification with the shortcuts that are listed in the top left part of the window and the options in the <code>Edit Feat_Positive</code> panel. </p> <p>Set the current class value to 2 by pressing i. Press Ctrl+right-click on a cell that you consider as Positive to set it to the 2nd class (Positive class, in light blue).</p> <p>Press i or d to set the current class value to 1 (negative cells, in maroon), and Control+right click on the cells that you want to set as Negative.</p> <p>You can see the table of each cell and its value for the feature (or other features already classified) in the <code>Features table</code> menu. Click on <code>Update table</code> to update it to the latest value of the new feature.</p> <p>Click on <code>Stop and Save</code> or on <code>Feat_Positive done</code> when you have finished to save the classification (in the result file with the other analysis).</p>"},{"location":"Threshold-channel/","title":"Threshold channel","text":"<p>Threshold a chosen channel/layer</p> <p>Select <code>Misc:Threshold channel</code> in the main pipeline interface to choose a channel/layer and the threshold to apply</p> <p>This option opens a panel to let you choose which channel you want to threshold. For flexibility, we let you choose any layer among the ones currently opened so that you could also threshold something else than the raw channel images.</p> <p>In <code>Threshold layer:</code> select the layer (from its name) that you want to threshold. The plugin will only show this layer and add a new layer called <code>Threshold</code>layername that contains the binary image after applying the threshold. The threshold layer is shown in white with transparency on top of the selected layer.</p> <p>Then slide the <code>Threshold</code> layer to choose the value of the threshold to apply. The display will change automatically as you slide it so you directly see the effect of your threshold choice. By default, the value is put to the mean of the image.</p> <p></p> <p>If you change the layer to threshold, or quit this option, the plugin doesn't close the threshold layer, so that it can be used in other part of the plugin.  If you wish to close it, do so manually by selecting it in the layers list and click on the trash icon.</p>"},{"location":"Touching-labels/","title":"Touching labels","text":"<p>Transforms the segmented cells to connected labels (touching)</p> <p>Select <code>Misc:Touching labels</code> in the action choice list to do this step</p> <p>In general after segmentation, the labels (the cells) are separated by one (or more) black pixels corresponding to the junctions. This option expands the labels so that they all touch, so that it can be loaded into other plugins like Griottes to generate the graph of the cells neighboring relationship.</p> <p></p>"},{"location":"Touching-labels/#napari-griottes-interoperability","title":"Napari-Griottes interoperability","text":"<p>This option allows to use the napari-Griottes plugin to generate spatial graph of cell relationships.</p> <p>To install this plugin, do:  <pre><code>    pip install napari-griottes\n</code></pre> in your virtual environment.</p> <p>To use the plugin, start it in <code>Plugins&gt;Griottes&gt;Make graph</code>. Choose the layer <code>TouchingCells</code> in the <code>label layer</code> parameter, and run it with the normal Griottes paramters.</p> <p>When the computation is finished, Griottes displays the results in the pixel scale.  As the images in <code>FishFeats</code> are scaled from their metadata, they might not be the same size in the display. To adjust the output of Griottes to the same scale as your data, click the <code>Scale Griottes image</code> in the <code>Touching labels</code> panel of FishFeats.</p> <p></p>"}]}